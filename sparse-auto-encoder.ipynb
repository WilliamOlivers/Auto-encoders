{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a simple sparse auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sparse Autoencoder\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=1e-3):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        # Encoder layer: Reduces input dimensions\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        # Decoder layer: Reconstructs input from encoded representation\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sparsity_lambda = sparsity_lambda  # Regularization strength\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode input using ReLU activation\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        # Decode back to input space\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def sparsity_loss(self, encoded):\n",
    "        # Compute average activation of hidden units\n",
    "        rho_hat = torch.mean(encoded, dim=0)\n",
    "        rho = 0.05  # Desired average activation\n",
    "        # KL divergence for sparsity constraint\n",
    "        return self.sparsity_lambda * torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 20  # Input feature dimension\n",
    "hidden_dim = 5   # Hidden layer size\n",
    "lr = 0.01        # Learning rate\n",
    "epochs = 100     # Number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Data (Random tensor simulating input data)\n",
    "X = torch.rand(100, input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4296\n",
      "Epoch 10, Loss: inf\n",
      "Epoch 20, Loss: inf\n",
      "Epoch 30, Loss: inf\n",
      "Epoch 40, Loss: inf\n",
      "Epoch 50, Loss: inf\n",
      "Epoch 60, Loss: inf\n",
      "Epoch 70, Loss: inf\n",
      "Epoch 80, Loss: inf\n",
      "Epoch 90, Loss: inf\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    encoded, decoded = model(X)  # Forward pass\n",
    "    loss = criterion(decoded, X) + model.sparsity_loss(encoded)  # Compute loss with sparsity constraint\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')  # Print progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-encoders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
