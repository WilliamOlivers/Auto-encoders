{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sparse auto-encoder to do TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sparse Autoencoder\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, sparsity_lambda=1e-3):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        # Encoder layer: Reduces input dimensions\n",
    "        self.encoder = nn.Linear(input_dim, hidden_dim)\n",
    "        # Decoder layer: Reconstructs input from encoded representation\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "        self.sparsity_lambda = sparsity_lambda  # Regularization strength\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode input using ReLU activation\n",
    "        encoded = torch.relu(self.encoder(x))\n",
    "        # Decode back to input space\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "    \n",
    "    def sparsity_loss(self, encoded):\n",
    "        # Compute average activation of hidden units\n",
    "        rho_hat = torch.mean(encoded, dim=0)\n",
    "        rho = 0.05  # Desired average activation\n",
    "        # KL divergence for sparsity constraint\n",
    "        return self.sparsity_lambda * torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 100  # Adjust based on TF-IDF vector size\n",
    "hidden_dim = 20   # Hidden layer size\n",
    "lr = 0.01        # Learning rate\n",
    "epochs = 100     # Number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "corpus = [\n",
    "    \"Machine learning is amazing\",\n",
    "    \"Deep learning is a subset of machine learning\",\n",
    "    \"Neural networks are powerful models\",\n",
    "    \"Autoencoders are used for unsupervised learning\",\n",
    "    \"Sparse autoencoders enforce sparsity constraints\",\n",
    "    \"Artificial intelligence is transforming industries\",\n",
    "    \"Data science involves statistics and programming\",\n",
    "    \"Natural language processing enables machines to understand text\",\n",
    "    \"Reinforcement learning is useful for decision making\",\n",
    "    \"Gradient descent is an optimization algorithm\",\n",
    "    \"Feature engineering improves machine learning models\",\n",
    "    \"Convolutional neural networks are used for image processing\",\n",
    "    \"Supervised learning uses labeled data\",\n",
    "    \"Unsupervised learning finds hidden patterns in data\",\n",
    "    \"Generative adversarial networks create new data\",\n",
    "    \"Support vector machines are powerful classifiers\",\n",
    "    \"Random forests are robust machine learning models\",\n",
    "    \"XGBoost is a popular gradient boosting algorithm\",\n",
    "    \"Overfitting occurs when a model learns noise\",\n",
    "    \"Underfitting happens when a model is too simple\",\n",
    "    \"Cross-validation helps assess model performance\",\n",
    "    \"Dimensionality reduction reduces the number of features\",\n",
    "    \"Principal component analysis is used for dimensionality reduction\",\n",
    "    \"K-means clustering groups data points into clusters\",\n",
    "    \"K-nearest neighbors is a simple classification algorithm\",\n",
    "    \"Decision trees are interpretable machine learning models\",\n",
    "    \"Bagging improves model stability by training multiple models\",\n",
    "    \"Boosting combines weak learners to create a strong learner\",\n",
    "    \"Hyperparameter tuning improves model performance\",\n",
    "    \"Data normalization scales features to a similar range\",\n",
    "    \"One-hot encoding is used for categorical variables\",\n",
    "    \"Transfer learning leverages pre-trained models\",\n",
    "    \"Recurrent neural networks are used for sequence data\",\n",
    "    \"Long short-term memory networks handle long-range dependencies\",\n",
    "    \"Word embeddings represent words as vectors\",\n",
    "    \"Attention mechanisms help models focus on important parts of input\",\n",
    "    \"BERT is a transformer-based model for NLP\",\n",
    "    \"GPT is a large language model for text generation\",\n",
    "    \"Explainable AI aims to make models more transparent\",\n",
    "    \"Clustering is an unsupervised learning technique\",\n",
    "    \"Model evaluation metrics include accuracy, precision, and recall\",\n",
    "    \"Data preprocessing is crucial for machine learning success\",\n",
    "    \"Outliers can negatively affect model performance\",\n",
    "    \"Ensemble methods combine predictions from multiple models\",\n",
    "    \"SVMs can handle both classification and regression tasks\",\n",
    "    \"Reinforcement learning agents learn by interacting with environments\",\n",
    "    \"Neural architecture search automates the design of neural networks\",\n",
    "    \"Model interpretability helps to understand decision-making\",\n",
    "    \"Self-supervised learning generates labels from the data itself\",\n",
    "    \"Data augmentation creates additional training data\",\n",
    "    \"Convolutional layers detect local patterns in images\",\n",
    "    \"Batch normalization improves training speed and stability\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(max_features=input_dim)\n",
    "X_tfidf = vectorizer.fit_transform(corpus).toarray()\n",
    "X = torch.tensor(X_tfidf, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "model = SparseAutoencoder(input_dim, hidden_dim)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0289\n",
      "Epoch 10, Loss: 0.0092\n",
      "Epoch 20, Loss: 0.0076\n",
      "Epoch 30, Loss: 0.0064\n",
      "Epoch 40, Loss: 0.0054\n",
      "Epoch 50, Loss: 0.0049\n",
      "Epoch 60, Loss: 0.0045\n",
      "Epoch 70, Loss: 0.0043\n",
      "Epoch 80, Loss: 0.0041\n",
      "Epoch 90, Loss: 0.0040\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    encoded, decoded = model(X)  # Forward pass\n",
    "    X = X.view(-1, input_dim)  # Ensure correct shape\n",
    "    loss = criterion(decoded, X) + model.sparsity_loss(encoded)  # Compute loss with sparsity constraint\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')  # Print progress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-encoders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
